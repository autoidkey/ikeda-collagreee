#!/usr/bin/env python## coding: utf-8import os, sysimport MeCabimport CaboChaimport csvimport mathimport reimport jsonimport requestsimport commands# from prettyprint import pp, pp_strfrom tools import tf_idffrom os import path#動作テスト用#pythonバインディングのMeCabとCaboChaが必要．#動作確認に便利なprettyprintを使用していますが，別途入れる必要があります．不要ならpp()の部分を削除してください．#元データから不要な部分（TF-IDF以外の重要語抽出方法とそれに関係する部分）を削りました．#これまではデータをcsvファイルでもらっていたので，csvを読み込むようにしていますが，適宜変更してください．#基本的に１行１スレッドが入っているcsvファイルにリメイクしてから使用します．（inputdataにサンプルあります）#ファイルの指定にpathを用いていますが，同じディレクトリ内であればファイル名で指定できます．#結果はoutputdataに出力されます．#読み込みファイルの用意# readcsv = open(path.abspath("inputdata") + "/remake.csv")#メカブとカボチャの用意m = MeCab.Tagger('-Owakati')c = CaboCha.Parser("")#単語ごとに区切られたものを文節として合成def word(tree, chunk, important_word):	result = ["", 0]	for ix  in range(chunk.token_pos,chunk.token_pos + chunk.token_size):#文節の初めから文節の終わりまで単語でループ		result[0] += tree.token(ix).surface		if tree.token(ix).surface in important_word:#重要語が含まれるならフラグを立てる			result[1] = 1	#print result[0]	return result#リスト内の空要素を削除def del_space(sentense_list):	flag = 0	while flag == 0:		try:			num = sentense_list.index("")			sentense_list.pop(num)		except:			flag = 1	return sentense_list#再帰で計算def linked(tree, status, linklist):	chunk = tree.chunk(status)	linklist.append(status)	if chunk.link != -1:		linked(tree, chunk.link, linklist)	return linklist#係り受け解析def kku(sentense, important_word):	result = ""	#pp(sentense)	tree =  c.parse(sentense)	wordlist = [] 	link = []	for i in range(tree.chunk_size()):#文節数でループ		chunk = tree.chunk(i)		w = word(tree, chunk, important_word)		wordlist.append(w[0])		if w[1] == 1:			link.append(i)	linklist = []	if link != []:		for l in link:			links = linked(tree, l, linklist)		for i in range(tree.chunk_size()):#文節数でループ			if i in links:				result += wordlist[i]	return result#出力先ファイルの指定1def output_file(option):	if option == "tf-idf":		return path.abspath("outputdata") + "/kruresult-tf-idf.txt"	else:		print "ファイルを指定してください"		sys.exit(1)#出力先ファイルの指定2def output_file2(option):	if option == "tf-idf":		return path.abspath("outputdata") + "/kruresult-tf-idf-important-word.txt"	else:		print "ファイルを指定してください"		sys.exit(1)#重要語抽出方法の指定def importance(option, all_sentense):	# writetxt = open(output_file2(option), "w")	all_list = all_sentense	if option == "tf-idf":		result = tf_idf.All_TF_IDF(all_list)	else:		print "抽出方法を指定してください"		sys.exit(1)	important_word = []	for num in result:		# writetxt.write(str(num) + "\n")		# for word in result[num]:			# writetxt.write(word + ": " + str(result[num][word]) + "\n")		if result[num] != {}:			sorted_dic = sorted(result[num].items(), key=lambda x:x[1], reverse=True)			for i in range(0, 2):				try:					important_word.append(sorted_dic[i][0])				except:					pass	#pp(important_word)	# writetxt.close()	return important_word#メインdef unit(option, all_sentense):	# writetxt = open(output_file(option), "w")	important_word = importance(option, all_sentense)#重要語の抽出	result = []	cnt = 0	for sentense in all_sentense:		cnt += 1		sent_len = len(sentense)		sentense = re.split(r'\(|\)|\!|\?|\.|。|「|」|！|？|（|）', sentense)		sentense = del_space(sentense)		w = ""		for s in sentense:			try:				sent = kku(s, important_word)				w += sent + " "			except:				sent = kku(s, important_word)				w += sent + " "		w = w.strip()		length = 1.0 * len(w) / sent_len		# writetxt.write(str(cnt) + ": " + str(length) + ": " + w + "\n")		#ここで出力させてruby側で撮ってくる		print w		# writetxt.write(w + "\n")		result.append(w)		#if cnt >= 2:		#	break	# pp(result)	# writetxt.close()#呼び出し時のメインif __name__ == "__main__":	argvs = sys.argv  # コマンドライン引数を格納したリストの取得	# print "aaaa"	# print str(argvs[1])	# print len(argvs)	# url = "http://localhost:3000/homes/" + str(argvs[1]) + "/json_user_entries"	# fetch_file = "test1.json"	# cmd = 'curl {} -o ./{}'.format(url, fetch_file)	# print commands.getstatusoutput(cmd)	del argvs[0]	useData = []	for data in argvs:		#print data		useData.append(data)		#print type(data.encode("utf-8"))	# json_data = json.load(open(argvs[1]))	# useData = []	# for data in json_data:	# 	if int(data["theme_id"])==int(sys.argv[1]):	# 		useData.append(data["body"].encode("utf-8"))	# 		print type(data["body"].encode("utf-8"))	# s = requests.get(url)	# for data in s.json():	# 	if int(data["theme_id"])==int(sys.argv[1]):	# 		useData.append(data["body"].encode("utf-8"))	# 		print type(data["body"].encode("utf-8"))	# # for row in csv.reader(readcsv):#スレッド単位	# #  	all_sentense.append(row[3])#ここで一つのリストに全てのスレッドを格納します		unit("tf-idf", useData)#重要語オプションとスレッドが入ったリストを引数とします